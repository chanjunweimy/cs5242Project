{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feature_pos = [[0,8],[8,12],[12,28],[28,44],[44,52],[52,64],[64,80],[80,92],[92,102]] \n",
    "df_train = pd.read_csv('train_kaggle.csv')\n",
    "df_test = pd.read_csv('sample_solution.csv')\n",
    "Y = df_train['Label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Frequency Features\n",
    "Skip this if you have saved the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataframes = np.load('allData.npy', allow_pickle = True)\n",
    "dataframes.shape\n",
    "\n",
    "def load_test_dataframe(id):\n",
    "    test_data = np.load(\"test/test/{}.npy\".format(id))\n",
    "    return pd.DataFrame(data=test_data)\n",
    "\n",
    "testdatas = []\n",
    "for id in df_test['Id']:\n",
    "    dfi = load_test_dataframe(id)\n",
    "    testdatas.append(dfi.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = dataframes[0].shape[1]\n",
    "# feat = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "def one_hot_encoding(encoder, dfs, col):\n",
    "    if encoder == None:\n",
    "        data = set(dfs[0][:,col])\n",
    "        for i in range(1,len(dfs)):\n",
    "            df = dfs[i]\n",
    "            data |= set(df[:,col])\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        encoder = encoder.fit([[str(e)] for e in data])\n",
    "    encodedData = []\n",
    "    for df in dfs:\n",
    "        mat = np.sum(encoder.transform([[str(e)] for e in df[:,col]]), axis=0)\n",
    "        mat = np.squeeze(np.asarray(mat))\n",
    "        encodedData.append(mat)\n",
    "    encodedData = np.stack(encodedData)    \n",
    "    return encoder, encodedData\n",
    "\n",
    "def one_hot_encoding_feature(encoders, dfs, feature):\n",
    "    if len(encoders) == 0:\n",
    "        encoder, data = one_hot_encoding(None, dfs, feature_pos[feature][0])\n",
    "        encoders.append(encoder)    \n",
    "        for col in tqdm(range(feature_pos[feature][0] + 1,feature_pos[feature][1])):\n",
    "            encoder, dataCol = one_hot_encoding(None, dfs, col)\n",
    "            encoders.append(encoder)\n",
    "            data = np.concatenate((data, dataCol), axis=1)\n",
    "    else:\n",
    "        encoder, data = one_hot_encoding(encoders[0], testdatas, feature_pos[feature][0])\n",
    "        for col in tqdm(range(feature_pos[feature][0] + 1,feature_pos[feature][1])):\n",
    "            i = col - feature_pos[feature][0]\n",
    "            encoder, dataCol = one_hot_encoding(encoders[i], testdatas, col)\n",
    "            data = np.concatenate((data, dataCol), axis=1)\n",
    "    return encoders, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 7/7 [03:24<00:00, 29.18s/it]\n",
      "100%|███████████████████████████████████████████████████████████| 7/7 [00:47<00:00,  6.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 3/3 [01:22<00:00, 27.61s/it]\n",
      "100%|███████████████████████████████████████████████████████████| 3/3 [00:22<00:00,  7.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████▊                                  | 6/15 [20:28<27:26, 182.96s/it]"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Give up last feature\n",
    "for feat in range(len(feature_pos) - 1):\n",
    "    print(feat)\n",
    "    encoders, train = one_hot_encoding_feature([], dataframes, feat)\n",
    "    encoders, test = one_hot_encoding_feature(encoders, testdatas, feat)\n",
    "    np.save('freq/' + str(feat) + '_train.npy', train)\n",
    "    np.save('freq/' + str(feat) + '_test.npy', test)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Frequency Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# last feature must be false\n",
    "feature_selected = [True,True,True,True,True,True,True,True,False] \n",
    "\n",
    "def load_freq(feat, postfix):\n",
    "    data = np.load(\"freq/{}_{}.npy\".format(feat, postfix))\n",
    "    return data\n",
    "\n",
    "def load_test_freq(feat):\n",
    "    return load_freq(feat, 'test')\n",
    "\n",
    "def load_train_freq(feat):\n",
    "    return load_freq(feat, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def select_features(feature_selected):\n",
    "    XTrain = []\n",
    "    XTest = []\n",
    "    for feat in tqdm(range(len(feature_pos))):\n",
    "        if feature_selected[feat] == False:\n",
    "            continue\n",
    "        train = load_train_freq(feat)\n",
    "        test = load_test_freq(feat)\n",
    "        XTrain.append(train)\n",
    "        XTest.append(test)\n",
    "    XTrain = np.concatenate(XTrain, axis=1)\n",
    "    XTest = np.concatenate(XTest, axis=1)\n",
    "    return XTrain, XTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "score = (0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "for f0 in range(2):\n",
    "    feature_selected[0] = bool(f0)\n",
    "    for f1 in range(2):\n",
    "        feature_selected[1] = bool(f1)\n",
    "        for f2 in range(2):\n",
    "            feature_selected[2] = bool(f2)\n",
    "            for f3 in range(2):\n",
    "                feature_selected[3] = bool(f3)\n",
    "                for f4 in range(2):\n",
    "                    feature_selected[4] = bool(f4)\n",
    "                    for f5 in range(2):\n",
    "                        feature_selected[5] = bool(f5)\n",
    "                        for f6 in range(2):\n",
    "                            feature_selected[6] = bool(f6)\n",
    "                            for f7 in range(2):\n",
    "                                feature_selected[7] = bool(f7)\n",
    "                                XTrain, XTest = select_features(feature_selected)\n",
    "                                X_train, X_valid, y_train, y_valid = train_test_split(XTrain, Y, test_size=0.2, random_state=42)\n",
    "                                model = LGBMClassifier()\n",
    "                                model.fit(X_train, y_train)\n",
    "                                pred = model.predict_proba(X_valid)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(y_valid, pred)\n",
    "                                auc_score = metrics.auc(fpr, tpr)\n",
    "                                if score[0] < auc_score:\n",
    "                                    score = (auc_score, f0, f1, f2, f3, f4, f5, f6, f7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    feature_selected[i] = bool(score[i + 1])\n",
    "\n",
    "XTrain, XTest = select_features(feature_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(XTrain, Y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train_set = lgb.Dataset(X_train.astype(np.int32), y_train.astype(int))\n",
    "valid_set = lgb.Dataset(X_valid.astype(np.int32), y_valid.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(XTrain, Y)\n",
    "modelSelection = SelectFromModel(clf, prefit=True, max_features=8000)\n",
    "XTrain = modelSelection.transform(XTrain)\n",
    "XTrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XTest = modelSelection.transform(XTest)\n",
    "XTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt import Trials\n",
    "from hyperopt import fmin\n",
    "from hyperopt import tpe\n",
    "import lightgbm as lgb\n",
    "from hyperopt import STATUS_OK\n",
    "\n",
    "ITER = 50\n",
    "STOP_ROUND = 5\n",
    "\n",
    "# Create the dataset\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['subsample_for_bin'] = int(params['subsample_for_bin'])\n",
    "    params['min_child_samples'] = int(params['min_child_samples'])\n",
    "    # Perform n_fold cross validation with hyperparameters\n",
    "    # Use early stopping and evalute based on ROC AUC\n",
    "    bst = lgb.train(params, train_set, ITER, valid_sets=valid_set, early_stopping_rounds=STOP_ROUND)\n",
    "    bst.save_model('model.txt', num_iteration=bst.best_iteration)\n",
    "  \n",
    "    # Extract the best score\n",
    "    best_score = bst.best_score['valid_0']['auc']\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'boosting_type': hp.choice('boosting_type', \n",
    "                                [\n",
    "                                    'gbdt',\n",
    "                                    'dart',\n",
    "                                    'goss'\n",
    "                                ]),\n",
    "    'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n",
    "    'metric': 'auc'\n",
    "}\n",
    "\n",
    "# Trials object to track progress\n",
    "bayes_trials = Trials()\n",
    "\n",
    "MAX_EVALS = 500\n",
    "\n",
    "# Optimize\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = bayes_trials)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best['boosting_type'] = 'dart'\n",
    "best['num_leaves'] = int(best['num_leaves'])\n",
    "best['subsample_for_bin'] = int(best['subsample_for_bin'])\n",
    "best['min_child_samples'] = int(best['min_child_samples'])\n",
    "lgbModel = lgb.train(best, train_set, ITER, valid_sets=valid_set, early_stopping_rounds=STOP_ROUND)\n",
    "lgbModel.save_model('model.txt', num_iteration=bst.best_iteration)\n",
    "YTest = lgbModel.predict(XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "model = LGBMClassifier(boosting_type=best['boosting_type'],\n",
    "                        num_leaves=best['num_leaves'],\n",
    "                        learning_rate=best['learning_rate'],\n",
    "                        subsample_for_bin=best['subsample_for_bin'],\n",
    "                        min_child_samples=best['min_child_samples'],\n",
    "                        reg_alpha=best['reg_alpha'],\n",
    "                        reg_lambda=best['reg_lambda'],\n",
    "                        colsample_bytree=best['colsample_by_tree'])\n",
    "model.fit(XTrain, Y)\n",
    "probs = model.predict_proba(XTest, num_iteration=lgbModel.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YTest = probs[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Predicted'] = YTest\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

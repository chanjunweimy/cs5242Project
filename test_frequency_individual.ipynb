{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feature_pos = [[0,8],[8,12],[12,28],[28,44],[44,52],[52,64],[64,80],[80,92],[92,102]] \n",
    "df_train = pd.read_csv('train_kaggle.csv')\n",
    "df_test = pd.read_csv('sample_solution.csv')\n",
    "Y = df_train['Label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Frequency Features\n",
    "Skip this if you have saved the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataframes = np.load('allData.npy', allow_pickle = True)\n",
    "dataframes.shape\n",
    "\n",
    "def load_test_dataframe(id):\n",
    "    test_data = np.load(\"test/test/{}.npy\".format(id))\n",
    "    return pd.DataFrame(data=test_data)\n",
    "\n",
    "testdatas = []\n",
    "for id in df_test['Id']:\n",
    "    dfi = load_test_dataframe(id)\n",
    "    testdatas.append(dfi.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = dataframes[0].shape[1]\n",
    "# feat = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "def one_hot_encoding(encoder, dfs, col):\n",
    "    if encoder == None:\n",
    "        data = set(dfs[0][:,col])\n",
    "        for i in range(1,len(dfs)):\n",
    "            df = dfs[i]\n",
    "            data |= set(df[:,col])\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        encoder = encoder.fit([[str(e)] for e in data])\n",
    "    encodedData = []\n",
    "    for df in dfs:\n",
    "        mat = np.sum(encoder.transform([[str(e)] for e in df[:,col]]), axis=0)\n",
    "        mat = np.squeeze(np.asarray(mat))\n",
    "        encodedData.append(mat)\n",
    "    encodedData = np.stack(encodedData)    \n",
    "    return encoder, encodedData\n",
    "\n",
    "def one_hot_encoding_feature(encoders, dfs, feature):\n",
    "    if len(encoders) == 0:\n",
    "        encoder, data = one_hot_encoding(None, dfs, feature_pos[feature][0])\n",
    "        encoders.append(encoder)    \n",
    "        for col in tqdm(range(feature_pos[feature][0] + 1,feature_pos[feature][1])):\n",
    "            encoder, dataCol = one_hot_encoding(None, dfs, col)\n",
    "            encoders.append(encoder)\n",
    "            data = np.concatenate((data, dataCol), axis=1)\n",
    "    else:\n",
    "        encoder, data = one_hot_encoding(encoders[0], testdatas, feature_pos[feature][0])\n",
    "        for col in tqdm(range(feature_pos[feature][0] + 1,feature_pos[feature][1])):\n",
    "            i = col - feature_pos[feature][0]\n",
    "            encoder, dataCol = one_hot_encoding(encoders[i], testdatas, col)\n",
    "            data = np.concatenate((data, dataCol), axis=1)\n",
    "    return encoders, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Give up last feature\n",
    "for feat in range(len(feature_pos) - 1):\n",
    "    print(feat)\n",
    "    encoders, train = one_hot_encoding_feature([], dataframes, feat)\n",
    "    encoders, test = one_hot_encoding_feature(encoders, testdatas, feat)\n",
    "    np.save('freq/' + str(feat) + '_train.npy', train)\n",
    "    np.save('freq/' + str(feat) + '_test.npy', test)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Frequency Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# last feature must be false\n",
    "feature_selected = [True,True,True,True,True,True,True,True,False] \n",
    "\n",
    "def load_freq(feat, postfix):\n",
    "    data = np.load(\"freq/{}_{}.npy\".format(feat, postfix))\n",
    "    return data\n",
    "\n",
    "def load_test_freq(feat):\n",
    "    return load_freq(feat, 'test')\n",
    "\n",
    "def load_train_freq(feat):\n",
    "    return load_freq(feat, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 9/9 [00:34<00:00,  3.82s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "XTrain = []\n",
    "XTest = []\n",
    "for feat in tqdm(range(len(feature_pos))):\n",
    "    if feature_selected[feat] == False:\n",
    "        continue\n",
    "    train = load_train_freq(feat)\n",
    "    test = load_test_freq(feat)\n",
    "    XTrain.append(train)\n",
    "    XTest.append(test)\n",
    "XTrain = np.concatenate(XTrain, axis=1)\n",
    "XTest = np.concatenate(XTest, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14929, 69548)\n",
      "(3733, 69548)\n",
      "(14929,)\n",
      "(3733,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(XTrain, Y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 34.,   3.,   0., ...,   0.,   0.,   0.],\n",
       "       [ 44.,  11.,   0., ...,   0.,   0.,   0.],\n",
       "       [119.,   6.,   0., ...,   0.,   0.,   0.],\n",
       "       ...,\n",
       "       [ 32.,   3.,   0., ...,   0.,   0.,   0.],\n",
       "       [ 34.,   3.,   0., ...,   0.,   0.,   0.],\n",
       "       [ 32.,   3.,   0., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train_set = lgb.Dataset(X_train.astype(np.int32), y_train.astype(int))\n",
    "valid_set = lgb.Dataset(X_valid.astype(np.int32), y_valid.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(XTrain, Y)\n",
    "modelSelection = SelectFromModel(clf, prefit=True, max_features=8000)\n",
    "XTrain = modelSelection.transform(XTrain)\n",
    "XTrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XTest = modelSelection.transform(XTest)\n",
    "XTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hyperopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-20274a24ca51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTrials\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtpe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hyperopt'"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt import Trials\n",
    "from hyperopt import fmin\n",
    "from hyperopt import tpe\n",
    "import lightgbm as lgb\n",
    "from hyperopt import STATUS_OK\n",
    "\n",
    "ITER = 50\n",
    "STOP_ROUND = 5\n",
    "\n",
    "# Create the dataset\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['subsample_for_bin'] = int(params['subsample_for_bin'])\n",
    "    params['min_child_samples'] = int(params['min_child_samples'])\n",
    "    # Perform n_fold cross validation with hyperparameters\n",
    "    # Use early stopping and evalute based on ROC AUC\n",
    "    bst = lgb.train(params, train_set, ITER, valid_sets=valid_set, early_stopping_rounds=STOP_ROUND)\n",
    "    bst.save_model('model.txt', num_iteration=bst.best_iteration)\n",
    "  \n",
    "    # Extract the best score\n",
    "    best_score = bst.best_score['valid_0']['auc']\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'boosting_type': hp.choice('boosting_type', \n",
    "                                [\n",
    "                                    'gbdt',\n",
    "                                    'dart',\n",
    "                                    'goss'\n",
    "                                ]),\n",
    "    'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n",
    "    'metric': 'auc'\n",
    "}\n",
    "\n",
    "# Trials object to track progress\n",
    "bayes_trials = Trials()\n",
    "\n",
    "MAX_EVALS = 500\n",
    "\n",
    "# Optimize\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = bayes_trials)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best['boosting_type'] = 'dart'\n",
    "best['num_leaves'] = int(best['num_leaves'])\n",
    "best['subsample_for_bin'] = int(best['subsample_for_bin'])\n",
    "best['min_child_samples'] = int(best['min_child_samples'])\n",
    "lgbModel = lgb.train(best, train_set, ITER, valid_sets=valid_set, early_stopping_rounds=STOP_ROUND)\n",
    "lgbModel.save_model('model.txt', num_iteration=bst.best_iteration)\n",
    "YTest = lgbModel.predict(XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "model = LGBMClassifier(boosting_type=best['boosting_type'],\n",
    "                        num_leaves=best['num_leaves'],\n",
    "                        learning_rate=best['learning_rate'],\n",
    "                        subsample_for_bin=best['subsample_for_bin'],\n",
    "                        min_child_samples=best['min_child_samples'],\n",
    "                        reg_alpha=best['reg_alpha'],\n",
    "                        reg_lambda=best['reg_lambda'],\n",
    "                        colsample_bytree=best['colsample_by_tree'])\n",
    "model.fit(XTrain, Y)\n",
    "probs = model.predict_proba(XTest, num_iteration=lgbModel.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "model = LGBMClassifier()\n",
    "model.fit(XTrain, Y)\n",
    "probs = model.predict_proba(XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "YTest = probs[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.007353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.998756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.014063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.508659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.903903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.999023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.916970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.001128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.929995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.013944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.954185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.999187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.996562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.894131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.001299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.894131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.994618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.008250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.997209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.863665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.001278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.001543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.997479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.898241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.998898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.030924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.894131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6021</th>\n",
       "      <td>6021</td>\n",
       "      <td>0.978520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6022</th>\n",
       "      <td>6022</td>\n",
       "      <td>0.943201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6023</th>\n",
       "      <td>6023</td>\n",
       "      <td>0.004750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6024</th>\n",
       "      <td>6024</td>\n",
       "      <td>0.000548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6025</th>\n",
       "      <td>6025</td>\n",
       "      <td>0.013617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6026</th>\n",
       "      <td>6026</td>\n",
       "      <td>0.904792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6027</th>\n",
       "      <td>6027</td>\n",
       "      <td>0.294195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6028</th>\n",
       "      <td>6028</td>\n",
       "      <td>0.000381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6029</th>\n",
       "      <td>6029</td>\n",
       "      <td>0.999058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6030</th>\n",
       "      <td>6030</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6031</th>\n",
       "      <td>6031</td>\n",
       "      <td>0.023555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6032</th>\n",
       "      <td>6032</td>\n",
       "      <td>0.009221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6033</th>\n",
       "      <td>6033</td>\n",
       "      <td>0.133913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6034</th>\n",
       "      <td>6034</td>\n",
       "      <td>0.878278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6035</th>\n",
       "      <td>6035</td>\n",
       "      <td>0.931710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6036</th>\n",
       "      <td>6036</td>\n",
       "      <td>0.996927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6037</td>\n",
       "      <td>0.000282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6038</th>\n",
       "      <td>6038</td>\n",
       "      <td>0.893918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6039</th>\n",
       "      <td>6039</td>\n",
       "      <td>0.005102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6040</th>\n",
       "      <td>6040</td>\n",
       "      <td>0.999370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>6041</td>\n",
       "      <td>0.974725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>6042</td>\n",
       "      <td>0.997332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>6043</td>\n",
       "      <td>0.999222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>6044</td>\n",
       "      <td>0.997606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>6045</td>\n",
       "      <td>0.515490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6046</th>\n",
       "      <td>6046</td>\n",
       "      <td>0.831043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6047</th>\n",
       "      <td>6047</td>\n",
       "      <td>0.001759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6048</th>\n",
       "      <td>6048</td>\n",
       "      <td>0.999378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6049</th>\n",
       "      <td>6049</td>\n",
       "      <td>0.670857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>6050</td>\n",
       "      <td>0.998936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6051 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  Predicted\n",
       "0        0   0.001261\n",
       "1        1   0.007353\n",
       "2        2   0.998756\n",
       "3        3   0.014063\n",
       "4        4   0.508659\n",
       "5        5   0.903903\n",
       "6        6   0.999023\n",
       "7        7   0.916970\n",
       "8        8   0.001128\n",
       "9        9   0.000531\n",
       "10      10   0.929995\n",
       "11      11   0.000182\n",
       "12      12   0.013944\n",
       "13      13   0.954185\n",
       "14      14   0.999187\n",
       "15      15   0.996562\n",
       "16      16   0.894131\n",
       "17      17   0.001299\n",
       "18      18   0.894131\n",
       "19      19   0.994618\n",
       "20      20   0.008250\n",
       "21      21   0.997209\n",
       "22      22   0.863665\n",
       "23      23   0.001278\n",
       "24      24   0.001543\n",
       "25      25   0.997479\n",
       "26      26   0.898241\n",
       "27      27   0.998898\n",
       "28      28   0.030924\n",
       "29      29   0.894131\n",
       "...    ...        ...\n",
       "6021  6021   0.978520\n",
       "6022  6022   0.943201\n",
       "6023  6023   0.004750\n",
       "6024  6024   0.000548\n",
       "6025  6025   0.013617\n",
       "6026  6026   0.904792\n",
       "6027  6027   0.294195\n",
       "6028  6028   0.000381\n",
       "6029  6029   0.999058\n",
       "6030  6030   0.002900\n",
       "6031  6031   0.023555\n",
       "6032  6032   0.009221\n",
       "6033  6033   0.133913\n",
       "6034  6034   0.878278\n",
       "6035  6035   0.931710\n",
       "6036  6036   0.996927\n",
       "6037  6037   0.000282\n",
       "6038  6038   0.893918\n",
       "6039  6039   0.005102\n",
       "6040  6040   0.999370\n",
       "6041  6041   0.974725\n",
       "6042  6042   0.997332\n",
       "6043  6043   0.999222\n",
       "6044  6044   0.997606\n",
       "6045  6045   0.515490\n",
       "6046  6046   0.831043\n",
       "6047  6047   0.001759\n",
       "6048  6048   0.999378\n",
       "6049  6049   0.670857\n",
       "6050  6050   0.998936\n",
       "\n",
       "[6051 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['Predicted'] = YTest\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
